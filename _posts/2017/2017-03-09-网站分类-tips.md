---
layout: post  
title: 网站分类项目中一些关键信息  
categories: 
- Project
tags:
- records ---

关于训练集和测试集的一些信息  
- 训练集中的合法网站在mongodb中最后的位置是343，之后需要用skip函数跳过这些记录  
- 训练集中的非法网站在mongodb中最后的位置是268，之后需要用skip函数跳过这些记录，mongo中总共的记录数目是1139  
- 重要特征，特征词在总词数中的占比,adult website(从训练集中看)是大于百分之四，而被误分类的正常网站是小于百分之四的,用3.5%,总词数要足够大  

**beautifulsoup** 的问题？？  
- title 为什么会解析成一长串呢？？

**mongodb**说明  
- origin_web:存放等待鉴别的网站webs  
- trimed_web:存放经过处理的网站信息  

#### 接下来方向-->
1. 如果能够判断日文网页，则翻译成中文，再进行操作  
    [\u3040-\u309F\u30A0-\u30FF]  
    [\u4e00-\u9fa5]  
    一个中文 一个日文
    使用这种正则匹配的方式进行判断前，必须将
2. 如果1行不通，则使用原来标注的网页进行模型优化，成人网站文本分类就到此为止，
3. 如果日文长度大于**50**，翻译合并,日文中的中文用使用的正则匹配方式是用中文匹配的，
4. 之后不能直接用百分比来判断特征词在总词数的占比，需要使用特征词的权重，所谓选取的特征词，实际上是默认非特征词在两类中出现的概率相等，实质上对在正常网站偏好出现的词是不公平的，但是凭经验看，正常词出现在两类词中是相等的（尤其是停用词）而这种词太多，不容易控制，概率也很小，对于样本集很少的情况下容易误导分类器，所以只选用特征词，在计算权重的时候非特征词的权重应该好好琢磨一下，直接使用训练出的词凭据作为权重还是不可靠,特定的词权重过大,导致对结果影响过大,有的权重达到四千多,开根号也不好控制  
    为什么在特征词特别少到几乎(或就是)零的情况下加权特征词密度还仍然很大呢??  
    计算特征加权密度时,如果该词在合法类中出现的较多就不参与计算,否则计算差值的对于2的对数,之后求和  
5. 先由自动标注的样本选出训练集，结合之前的集合，还有测试集合

##### 页面跳转,匹配url到新网页,


#### 仿冒钓鱼网站识别
1. 基本方案确定:  
- [使用bigml](https://github.com/rishy/phishing-websites)
- 使用朴素贝叶斯,非平衡SVM
    
