---
layout: post  
title: 机器学习-读书笔记  
category: 
- ML  
tags:
- records
---

#### 第六章　支持向量机
- 函数间隔的概念：y(**w**x+b),函数间隔，正负表示分类的正确性，大小表示分类正确的确性度,将法向量||w||限制为||w||==1之后，函数间隔就变成了几何间隔，这样做的目的是规范化了，法向量的模长，（规范化总是需要的，可以省去很多不必要的麻烦）｀
- 最大间隔分离超平面存在且唯一，唯一性证明（回头再看，ｓｖｍ看完再复习)  
- 关键是要弄懂优化方法，将原始问题化为对偶问题，利用KTT条件，使用拉格朗日乘子法，找出最值点，带入解得原始最优解（（关键））
- 问题：如何判断数据集是不是线性可分的，或者已知训练数据集是线性可分的，那么如何找到导致其线性不可分的特异点（噪声）呢，**???***,如何判断是不是近似线性可分？？
- 核函数选择的有效性需要通过实验验证,映射能够构成向量空间,并定义内积构成内积空间,最后完备化成希尔伯特空间的必要条件是,相应的核函数(??)的Gram矩阵是半正定的??,半正定的证明??  
  字符串核函数可以由**动态规划**快速的计算

#### 集成学习
1. 如何发现弱学习算法？  
  
2. adaboost   
- 弱分类器的系数是怎么计算出的，为什么要这样计算？目的：最小化**指数损失函数**  
- 迭代生成弱分类器过程中错误率大于0.5，break 之后怎么办？？？--->>抛弃当前的基学习器,且学习过程停止,若没有达到初始设置的学习轮数,导致基学习器很少而性能不佳,可采用**重采样法**获得'重启动'机会,避免训练过程过早停止.  
- 指数损失函数与0-1损失函数效果一致的**证明**
- 每次新分类器系数得到的方法-->使得最小化指数损失函数（因为总分类器是加权和，所以可以这样做）
- zm 怎么推导出的，watermelon，P176，最后一步
- 不需要知道下界？？，下界为零？？，lihangP143

#### Bagging与随机森林
1. 如果采样出的每个子集都完全不同，每个基学习器都只用到一小部分训练数据，便不能进行有效的学习--->> 特定类型样本的预测不能通过不同的基学习器共同作用来实现，也就失去了提升（boost）的目的，--->> 采用相互有交叠的采样子集，（但是为了体现差异，又不能交叠太多！！-->> 如何把握**?**）
2. **63.2%** ?? -->  如何按照需求调整？重复的怎么办？
3. 决策树：
    - 注意标记叶节点的方式，是该节点还是该节点的父节点所含样本最多的类别？？
    - **C4.5**算法不是使用增益率最大的属性，而是实用了一个启发式：从高于平均增益的属性中选择增益率最高的
    - 数据集的纯度除了用熵表示，还能用基尼指数度量:CART决策树
    - 划分节点带来泛化性能的提升，怎么度量？？<---> 与过拟合成双成对,这里采用**留出法**，是否有其他方法？？？
    - 剪枝策略：预剪枝，需要留出验证集合，使用某个属性进行划分前在验证集上计算精度，划分后计算精度，比较会不会带来验证集上的精度提升（泛化能力提升），是：使用该属性划分，否:剪枝

4. 随机森林
    - 核心：样本扰动，属性扰动（样本扰动是因为RF构建是在Bagging的基础上）
    - UCI数据
    - 投票法：P183
        - 对于能够在产生类标记同时产生置信度的学习器，其分类置信度可转化为类概率使用，可能未进行规范化（eg：svm的分类间隔值，）需要使用一些技术（eg：Platter缩放，等分回归等）进行校准后才能最为类概率使用。
        - 若基学习器类型不同，则类概率值不能直接进行比较---->转化为类标记再进行投票？？为什么
