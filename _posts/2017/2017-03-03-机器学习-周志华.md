---
layout: post  
title: 机器学习-读书笔记  
category: 
- ML  
tags:
- records
---

#### 集成学习
1. 如何发现弱学习算法？  
  
2. adaboost   
- 弱分类器的系数是怎么计算出的，为什么要这样计算？目的：最小化**指数损失函数**  
- 迭代生成弱分类器过程中错误率大于0.5，break 之后怎么办？？？--->>抛弃当前的基学习器  
- 指数损失函数与0-1损失函数效果一致的**证明**
- 每次新分类器系数得到的方法-->使得最小化指数损失函数（因为总分类器是加权和，所以可以这样做）
- zm 怎么推导出的，watermelon，P176，最后一步
- 不需要知道下界？？，下界为零？？，lihangP143

#### Bagging与随机森林
1. 如果采样出的每个子集都完全不同，每个基学习器都只用到一小部分训练数据，便不能进行有效的学习--->> 特定类型样本的预测不能通过不同的基学习器共同作用来实现，也就失去了提升（boost）的目的，--->> 采用相互有交叠的采样子集，（但是为了体现差异，又不能交叠太多！！-->> 如何把握**?**）
2. **63.2%** ?? -->  如何按照需求调整？重复的怎么办？
3. 决策树：
    - 注意标记叶节点的方式，是该节点还是该节点的父节点所含样本最多的类别？？
    - **C4.5**算法不是使用增益率最大的属性，而是实用了一个启发式：从高于平均增益的属性中选择增益率最高的
    - 数据集的纯度除了用熵表示，还能用基尼指数度量:CART决策树
    - 划分节点带来泛化性能的提升，怎么度量？？<---> 与过拟合成双成对,这里采用**留出法**，是否有其他方法？？？

4. 随机森林
    - 核心：样本扰动，属性扰动（样本扰动是因为RF构建是在Bagging的基础上）
    - UCI数据
    - 投票法：P183
        - 对于能够在产生类标记同时产生置信度的学习器，其分类置信度可转化为类概率使用，可能未进行规范化（eg：svm的分类间隔值，）需要使用一些技术（eg：Platter缩放，等分回归等）进行校准后才能最为类概率使用。
        - 若基学习器类型不同，则类概率值不能直接进行比较---->转化为类标记再进行投票？？为什么
